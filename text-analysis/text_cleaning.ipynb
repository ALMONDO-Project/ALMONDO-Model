{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\valen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\valen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\valen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import demoji\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('translated.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean tweets\n",
    "def clean_text(row):\n",
    "    \n",
    "    def remove_emoji(text):\n",
    "        for k in demoji.findall(text):\n",
    "            text = text.replace(k, '')\n",
    "        return text\n",
    "    \n",
    "    text = row['text']\n",
    "    username = row['username']\n",
    "       \n",
    "    # Remove user mentions (words starting with \"@\")\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    \n",
    "    # Remove user mentions (words starting with \"@\")\n",
    "    text = re.sub(r'#\\w+', ' ', text)\n",
    "    \n",
    "    # Remove the username of the owner of the tweet\n",
    "    text = re.sub(rf'\\b{username}\\b', ' ', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    #remove emoji\n",
    "    text = remove_emoji(text)\n",
    "       \n",
    "    # Remove contractions\n",
    "    contractions_pattern = re.compile(r'\\b(?:can\\'t|cannot|couldn\\'t|aren\\'t|they\\'re|you\\'re|we\\'re|I\\'m|won\\'t|wouldn\\'t|shouldn\\'t|don\\'t|doesn\\'t|didn\\'t|haven\\'t|hasn\\'t|hadn\\'t|isn\\'t|it\\'s|wasn\\'t|weren\\'t|what\\'s|that\\'s|there\\'s|where\\'s|who\\'s|why\\'s|how\\'s|i\\'ll|you\\'ll|he\\'ll|she\\'ll|we\\'ll|they\\'ll|i\\'d|you\\'d|he\\'d|she\\'d|we\\'d|they\\'d|let\\'s|ain\\'t|mustn\\'t|needn\\'t|shan\\'t|can\\'ve|could\\'ve|may\\'ve|might\\'ve|must\\'ve|shall\\'ve|should\\'ve|would\\'ve|mightn\\'t|oughtn\\'t|daren\\'t|didn\\'t|haven\\'t|hasn\\'t|hadn\\'t|isn\\'t|wasn\\'t|weren\\'t|can\\'t|couldn\\'t|daren\\'t|hadn\\'t|hasn\\'t|haven\\'t|isn\\'t|mightn\\'t|mustn\\'t|needn\\'t|shan\\'t|shouldn\\'t|won\\'t|wouldn\\'t)\\b', re.IGNORECASE)\n",
    "    text = re.sub(contractions_pattern, ' ', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "    \n",
    "    # Remove single and double quotes\n",
    "    text = re.sub(r\"[\\\"\\']\", ' ', text)\n",
    "    \n",
    "    text = text.replace('\"', ' ')\n",
    "    text = text.replace(\"'\", ' ')\n",
    "    text = text.replace('“', ' ')\n",
    "    text = text.replace(\"’\", \" \")\n",
    "    text = text.replace('”', \" \")\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    # Remove newlines, tabs, and extra whitespace\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "    # Remove stop words\n",
    "    stop_words = (stopwords.words('english'))\n",
    "    stop_words.extend(['nordisk', 'nordisks', 'novo', 'abb', 'schneider', 'lego', 'hi', 'hello',\n",
    "    'novorossiysk', 'enel', 'https', 'every', 'single', 'enso', 'acciona', 'akzo', 'today', 'tomorrow',\n",
    "    'week', 'month', 'whyee', 'june', 'without', 'with', 'last', 'first', 'info', 'good', 'bad', 'little', 'big',\n",
    "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve',\n",
    "    'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen', 'twenty', 'thirty',\n",
    "    'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'thousand', 'million', 'billion', 'trillion',\n",
    "    'january', 'february', 'march', 'april', 'may', 'july', 'august', 'september', 'october', 'november', 'december',\n",
    "    'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',\n",
    "    'other', 'same', 'different', 'new', 'old', 'young', 'long', 'short', 'right', 'wrong', 'high', 'low',\n",
    "    'early', 'late', 'strong', 'weak', 'happy', 'sad', 'easy', 'difficult', 'hard', 'soft', 'big', 'small',\n",
    "    'very', 'too', 'also', 'well', 'quickly', 'slowly', 'easily', 'hardly', 'almost', 'nearly', 'always',\n",
    "    'never', 'sometimes', 'often', 'usually', 'now', 'then', 'here', 'there', 'together', 'apart',\n",
    "    'and', 'or', 'but', 'nor', 'so', 'yet', 'for', 'xniwrqo', 'wxyk', 'zkdwud', 'dihc', 'jlgt',\n",
    "    'of', 'to', 'in', 'on', 'at', 'by', 'with', 'about', 'against', 'between', 'into', 'through', 'during',\n",
    "    'before', 'after', 'above', 'below', 'from', 'up', 'down', 'under', 'over', 'again', 'further',\n",
    "    'i', 'me', 'you', 'he', 'him', 'she', 'her', 'it', 'we', 'us', 'they', 'them', 'my', 'your', 'his',\n",
    "    'her', 'its', 'our', 'their', 'mine', 'yours', 'hers', 'ours', 'theirs', 'abbformulae', 'smnsj',\n",
    "    'a', 'an', 'the', 'is', 'am', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',\n",
    "    'would', 'shall', 'ogpj', 'should', 'can', '恭喜发财', 'uaigkgunt', 'could','ycky', 'may', 'might', 'must', 'who', 'what', 'when', 'where', 'why', 'how', 'which', 'that', 'this', 'these', 'those', 'there', 'here', 'rjwpjiyti'])\n",
    "    \n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 3]\n",
    "\n",
    "    \n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    words = [word for word in words if row['username'].lower() not in word]\n",
    "    words = [word for word in words if 'http' not in word]\n",
    "    words = [word for word in words if not word.startswith('abb')]\n",
    "    words = [word for word in words if not word.startswith('zmx')]\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 3]\n",
    "        \n",
    "    # Join words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df.apply(clean_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        emobility home charging solution help driver r...\n",
       "1        facilitate saving formula ability optimax ener...\n",
       "2        better decision greener data center energy sol...\n",
       "3        formula season green mexico formula world cham...\n",
       "4        decoded frank muehlon emobilitys discus chargi...\n",
       "                               ...                        \n",
       "10557    sorry hear cristian please send direct message...\n",
       "10558    sorry hear please send direct message require ...\n",
       "10559    sorry hear please send direct message require ...\n",
       "10560    sorry hear please send direct message assist jade\n",
       "10561    recommend touch nearest volvo approved service...\n",
       "Name: cleaned_text, Length: 10333, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['username', 'created_at', 'cleaned_text']].to_csv('cleaned_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
