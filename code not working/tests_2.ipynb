{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import collect_results, load_credentials, gen_request_parameters\n",
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd\n",
    "import json as serializer\n",
    "import requests\n",
    "import os\n",
    "from os.path import join as join, dirname\n",
    "import smtplib, ssl\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Get today's date\n",
    "today = date.today()\n",
    "\n",
    "\n",
    "base = \"C:/Users/valen/GitHub/almondo-tweets-retrieval\"\n",
    "\n",
    "class UserDataDownloader():\n",
    "    def __init__(self, query, username='', tweet_fields='text,id,attachments,created_at,lang,author_id,entities,geo', expansions='attachments.media_keys,geo.place_id', media_fields='media_key,type,url,variants,preview_image_url'):\n",
    "        # If i only want the tweet text i can use tweet_fields='' or tweet_fields=None\n",
    "        # Both will send no tweet fields, and you'll receive only the default fields (id, text, edit_history_tweet_ids)\n",
    "        if tweet_fields == '':\n",
    "            tweet_fields = None\n",
    "\n",
    "        self.username = username\n",
    "        self.filename = f\"{username}\"\n",
    "        # Filename for the day\n",
    "        \n",
    "        self.path = f\"{base}/data\"\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "        \n",
    "        self.tweets_path = f\"{self.path}/out\"\n",
    "        if not self.tweets_path:\n",
    "            self.tweets_path = f\"{self.path}/out\"\n",
    "        \n",
    "        if not os.path.exists(self.tweets_path):\n",
    "            os.makedirs(self.tweets_path)\n",
    "            \n",
    "        self.search_credentials = load_credentials(filename=f\"{base}\\cred.yaml\", yaml_key=\"search_tweets_cred\")\n",
    "        self.counts_credentials = load_credentials(filename=f\"{base}\\cred.yaml\", yaml_key=\"counts_tweets_cred\")\n",
    "        self.search_rule = gen_request_parameters(query,\n",
    "                                        results_per_call=100,\n",
    "                                        tweet_fields=tweet_fields,\n",
    "                                        media_fields=media_fields,\n",
    "                                        expansions=expansions,\n",
    "                                        start_time=(datetime(year=2023, month=1, day=1)).strftime(\"%Y-%m-%d\"),\n",
    "                                        end_time=(datetime(year=2023, month=12, day=31)).strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "        self.counts_rule = gen_request_parameters(query)\n",
    "\n",
    "    def update_api_limits(self, day_downloded):\n",
    "        print('Updating api limits: ')\n",
    "        def check_usage():\n",
    "            cred = load_credentials(filename=f\"{base}/cred.yaml\", yaml_key=\"usage_cred\")\n",
    "            \n",
    "            def bearer_oauth(r):\n",
    "                \"\"\"\n",
    "                Method required by bearer token authentication.\n",
    "                \"\"\"\n",
    "\n",
    "                r.headers[\"Authorization\"] = f\"Bearer {cred['bearer_token']}\"\n",
    "                r.headers[\"User-Agent\"] = \"v2UsageTweetsPython\"\n",
    "                return r\n",
    "            \n",
    "            response = requests.request('GET', url=cred['endpoint'], auth=bearer_oauth)\n",
    "            if response.status_code != 200:\n",
    "                raise Exception(\n",
    "                    \"Request returned an error: {} {}\".format(\n",
    "                        response.status_code, response.text\n",
    "                    )\n",
    "                )\n",
    "            return response.json()\n",
    "        \n",
    "        usage = check_usage()['data']\n",
    "        # Check the api limits\n",
    "        counts = collect_results(self.counts_rule, result_stream_args=self.counts_credentials)\n",
    "        if os.path.exists(f\"{self.path}/log/limits.json\"):\n",
    "            with open(f\"{self.path}/log/limits.json\", \"r\") as f:\n",
    "                data = serializer.load(f)\n",
    "        else:\n",
    "            data = {}    \n",
    "            data['User'] = self.username\n",
    "            data['Remaining_Month_Cap_Limit'] = int(int(usage['project_cap']) - int(usage['project_usage']))\n",
    "            \n",
    "        data['Month_Cap_Limit'] = int(usage['project_cap'])\n",
    "        data['Used_Requests'] = int(usage['project_usage']) + int(counts[0]['data'][0]['tweet_count'])\n",
    "        data['Cap_Reset_Day'] = int(usage['cap_reset_day'])\n",
    "        \n",
    "        data['Remaining_Month_Cap_Limit'] = int(data['Month_Cap_Limit']) - int(data['Used_Requests'])\n",
    "        \n",
    "        if today.day > data['Cap_Reset_Day']:\n",
    "            data['Cap_Remaining_Days'] = int((date(today.year, today.month+1, int(data['Cap_Reset_Day'])) - today).days)\n",
    "        else:\n",
    "            data['Cap_Remaining_Days'] = int((date(today.year, today.month, int(data['Cap_Reset_Day'])) - today).days)\n",
    "        \n",
    "        data['User_Cap_Limit'] = int(int(data['Remaining_Month_Cap_Limit']) - 1000)\n",
    "        \n",
    "        self.max_tweets = int(data['User_Cap_Limit'])\n",
    "        # Print the limits\n",
    "        print('#########################################################')\n",
    "        print(data)\n",
    "        print('#########################################################')\n",
    "        # Save the limits\n",
    "        \n",
    "        day_donwloading = (today - timedelta(day_downloded)).strftime(\"%Y-%m-%d\")\n",
    "        serializer.dump(data, open(f\"{self.path}/log/limits.json\", \"w\"))\n",
    "        serializer.dump(counts, open(f\"{self.path}/counts/{str(today)}_counts_{day_donwloading}.json\", \"w\"))\n",
    "    \n",
    "    def join_and_save(self):\n",
    "        all_tweets_json = []\n",
    "        for file in os.listdir(f\"{self.path}tweets/\"):\n",
    "            if file.endswith(\".json\"):\n",
    "                file = serializer.loads(open(f\"{self.path}tweets/{file}\").read())\n",
    "                all_tweets_json.append(file)\n",
    "        all_tweets_json = [tweet for tweets in all_tweets_json for tweet in tweets]\n",
    "        serializer.dump(all_tweets_json, open(f\"{self.path}all_tweets.json\", \"w\"))\n",
    "\n",
    "    def download_and_save(self):\n",
    "        # self.update_api_limits(self.day_downloded)\n",
    "        # Collecting tweets\n",
    "        print(f\"Collecting tweets for user {self.filename}\")\n",
    "        tweets = collect_results(self.search_rule, max_tweets=self.max_tweets, result_stream_args=self.search_credentials)\n",
    "        print(f\"Dumping data...\")\n",
    "        serializer.dump(tweets, open(f\"{self.tweets_path}{self.filename}.json\", \"w\"))\n",
    "        try:\n",
    "            self.join_and_save()\n",
    "        except:\n",
    "            print(\"Can't join the data\")\n",
    "            pass\n",
    "        print(f\"Done.\\n\")\n",
    "        \n",
    "\n",
    "        \n",
    "downloader = UserDataDownloader(day=1\n",
    "                            , query=\"from:\")\n",
    "downloader.download_and_save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
